{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
      "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import dedupe\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEDUPE_2_ERS_DATAFIELD_MAPPING = {'abs_name':'name', 'abs_legal_form':'legal_form',\n",
    "                                'abs_register_number':'register_number', 'abs_hq_email':'email', 'abs_website':'website',\n",
    "                                'abs_hq_phone':'phone_number', 'abs_taxid':'vat_id',\n",
    "                                'abs_hq_street':'address.street', 'abs_hq_zip_code':'address.postal_code',\n",
    "                                'abs_hq_city':'address.city', 'abs_hq_country':'address.country',\n",
    "                                'record_id':'record_id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/dedupe/sampling.py:39: UserWarning: 7500 blocked samples were requested, but only able to sample 6274\n",
      "  % (sample_size, len(blocked_sample)))\n",
      "INFO:dedupe.api:reading training from file\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, abs_name), SimplePredicate: (sameThreeCharStartPredicate, abs_name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonSixGram, abs_hq_city), SimplePredicate: (commonTwoTokens, abs_hq_street))\n",
      "INFO:dedupe.training:(SimplePredicate: (sameThreeCharStartPredicate, abs_name), SimplePredicate: (twoGramFingerprint, abs_hq_country))\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.100000, score 0.6938146892556963\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameThreeCharStartPredicate, abs_name), SimplePredicate: (suffixArray, abs_hq_zip_code))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonTwoTokens, abs_name), SimplePredicate: (sameThreeCharStartPredicate, abs_name))\n",
      "INFO:dedupe.training:(SimplePredicate: (sameThreeCharStartPredicate, abs_name), SimplePredicate: (twoGramFingerprint, abs_hq_country))\n"
     ]
    }
   ],
   "source": [
    "trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / '2018-07-03-trainings_data_from_dedupe.json')\n",
    "with open(trainset_filepath) as file:\n",
    "    trainset = json.load(file)\n",
    "    \n",
    "with open(str(Path(os.getenv('MODELS_PATH')) / '2018-06-26-dedupeio_datamodel.json')) as file:\n",
    "    fields = json.load(file)\n",
    "\n",
    "for field in fields:\n",
    "    field['field'] = DEDUPE_2_ERS_DATAFIELD_MAPPING[field['field']]\n",
    "\n",
    "for [a, b] in trainset['match']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "for [a, b] in trainset['distinct']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "\n",
    "# messy = {('d'+str(k)):v[0] for k,v in zip(range(len(trainset['distinct'])), trainset['distinct'])}\n",
    "# canonical = {('m'+str(k)):v[0] for k,v in zip(range(len(trainset['match'])), trainset['match'])}\n",
    "\n",
    "# somehow the data must be split up: one part has to be use to call this gazetteer.sample function,\n",
    "# the other part is used as the \"real\" trainings data:\n",
    "messy = {('d'+str(k)):v[0] for k,v in zip(range(100), trainset['distinct'])}\n",
    "canonical = {('m'+str(k)):v[0] for k,v in zip(range(100), trainset['match'])}\n",
    "\n",
    "new_trainset = {'match':trainset['match'][101:], 'distinct':trainset['distinct'][101:]}\n",
    "new_trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / 'new_trainset.json')\n",
    "with open(new_trainset_filepath, 'w') as fp:\n",
    "    json.dump(new_trainset, fp)\n",
    "\n",
    "gazetteer = dedupe.Gazetteer(fields)\n",
    "gazetteer.sample(messy, canonical)\n",
    "\n",
    "with open(new_trainset_filepath) as tf:\n",
    "    gazetteer.readTraining(tf)\n",
    "    \n",
    "gazetteer.train()\n",
    "\n",
    "model_output_filepath = str(Path(os.getenv('MODELS_PATH')) / 'very_first.model')\n",
    "with open(model_output_filepath, 'wb') as sf:\n",
    "    gazetteer.writeSettings(sf, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/dedupe/sampling.py:39: UserWarning: 7500 blocked samples were requested, but only able to sample 1672\n",
      "  % (sample_size, len(blocked_sample)))\n",
      "INFO:dedupe.api:reading training from file\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (sameThreeCharStartPredicate, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonFourGram, address.street), SimplePredicate: (nearIntegersPredicate, address.postal_code))\n",
      "INFO:dedupe.training:(SimplePredicate: (fingerprint, address.country), SimplePredicate: (sameThreeCharStartPredicate, name))\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.100000, score 0.6073378014501305\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (sameThreeCharStartPredicate, name))\n"
     ]
    }
   ],
   "source": [
    "trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / '2018-07-03-trainings_data_from_dedupe.json')\n",
    "with open(trainset_filepath) as file:\n",
    "    trainset = json.load(file)\n",
    "    \n",
    "with open(str(Path(os.getenv('MODELS_PATH')) / '2018-06-26-dedupeio_datamodel.json')) as file:\n",
    "    fields = json.load(file)\n",
    "    \n",
    "for field in fields:\n",
    "    field['field'] = DEDUPE_2_ERS_DATAFIELD_MAPPING[field['field']]\n",
    "\n",
    "for [a, b] in trainset['match']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "for [a, b] in trainset['distinct']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "\n",
    "# messy = {('d'+str(k)):v[0] for k,v in zip(range(len(trainset['distinct'])), trainset['distinct'])}\n",
    "# canonical = {('m'+str(k)):v[0] for k,v in zip(range(len(trainset['match'])), trainset['match'])}\n",
    "\n",
    "# somehow the data must be split up: one part has to be use to call this gazetteer.sample function,\n",
    "# the other part is used as the \"real\" trainings data:\n",
    "messy = {('d'+str(k)):v[0] for k,v in zip(range(50), trainset['distinct'])}\n",
    "canonical = {('m'+str(k)):v[0] for k,v in zip(range(50), trainset['match'])}\n",
    "\n",
    "new_trainset = {'match':trainset['match'][51:], 'distinct':trainset['distinct'][51:]}\n",
    "new_trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / 'snd_new_trainset.json')\n",
    "with open(new_trainset_filepath, 'w') as fp:\n",
    "    json.dump(new_trainset, fp)\n",
    "    \n",
    "gazetteer = dedupe.Gazetteer(fields)\n",
    "gazetteer.sample(messy, canonical)\n",
    "\n",
    "with open(new_trainset_filepath) as tf:\n",
    "    gazetteer.readTraining(tf)\n",
    "    \n",
    "gazetteer.train(recall=.05)\n",
    "\n",
    "model_output_filepath = str(Path(os.getenv('MODELS_PATH')) / 'second.model')\n",
    "with open(model_output_filepath, 'wb') as sf:\n",
    "    gazetteer.writeSettings(sf, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/dedupe/sampling.py:39: UserWarning: 7500 blocked samples were requested, but only able to sample 1646\n",
      "  % (sample_size, len(blocked_sample)))\n",
      "INFO:dedupe.api:reading training from file\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (sameThreeCharStartPredicate, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonFourGram, address.street), SimplePredicate: (nearIntegersPredicate, address.postal_code))\n",
      "INFO:dedupe.training:(SimplePredicate: (sameThreeCharStartPredicate, name), SimplePredicate: (sortedAcronym, address.country))\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.100000, score 0.5819708628105468\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (sameThreeCharStartPredicate, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonFourGram, address.street), SimplePredicate: (nearIntegersPredicate, address.postal_code))\n"
     ]
    }
   ],
   "source": [
    "trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / '2018-07-03-trainings_data_from_dedupe.json')\n",
    "with open(trainset_filepath) as file:\n",
    "    trainset = json.load(file)\n",
    "    \n",
    "with open(str(Path(os.getenv('MODELS_PATH')) / '2018-06-26-dedupeio_datamodel.json')) as file:\n",
    "    fields = json.load(file)\n",
    "    \n",
    "for field in fields:\n",
    "    field['field'] = DEDUPE_2_ERS_DATAFIELD_MAPPING[field['field']]\n",
    "\n",
    "for [a, b] in trainset['match']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "for [a, b] in trainset['distinct']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "\n",
    "# messy = {('d'+str(k)):v[0] for k,v in zip(range(len(trainset['distinct'])), trainset['distinct'])}\n",
    "# canonical = {('m'+str(k)):v[0] for k,v in zip(range(len(trainset['match'])), trainset['match'])}\n",
    "\n",
    "# somehow the data must be split up: one part has to be use to call this gazetteer.sample function,\n",
    "# the other part is used as the \"real\" trainings data:\n",
    "messy = {('d'+str(k)):v[0] for k,v in zip(range(50), trainset['distinct'])}\n",
    "canonical = {('m'+str(k)):v[0] for k,v in zip(range(50), trainset['match'])}\n",
    "\n",
    "new_trainset = {'match':trainset['match'][51:], 'distinct':trainset['distinct'][51:]}\n",
    "new_trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / 'snd_new_trainset.json')\n",
    "with open(new_trainset_filepath, 'w') as fp:\n",
    "    json.dump(new_trainset, fp)\n",
    "    \n",
    "gazetteer = dedupe.Gazetteer(fields)\n",
    "gazetteer.sample(messy, canonical)\n",
    "\n",
    "with open(new_trainset_filepath) as tf:\n",
    "    gazetteer.readTraining(tf)\n",
    "    \n",
    "gazetteer.train(recall=.95)\n",
    "\n",
    "model_output_filepath = str(Path(os.getenv('MODELS_PATH')) / 'third.model')\n",
    "with open(model_output_filepath, 'wb') as sf:\n",
    "    gazetteer.writeSettings(sf, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trained on big data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getenv('DATA_PATH') + '/training-data-from-kantwert-buergle/training_data.json') as file:\n",
    "    trainset = json.load(file)\n",
    "\n",
    "with open(str(Path(os.getenv('MODELS_PATH')) / '2018-06-26-dedupeio_datamodel.json')) as file:\n",
    "    fields = json.load(file)\n",
    "\n",
    "for field in fields:\n",
    "    field['field'] = DEDUPE_2_ERS_DATAFIELD_MAPPING[field['field']]\n",
    "\n",
    "for [a, b] in trainset['match']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)\n",
    "for [a, b] in trainset['distinct']:\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        a[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = a.pop(field)\n",
    "    for field in DEDUPE_2_ERS_DATAFIELD_MAPPING.keys():\n",
    "        b[DEDUPE_2_ERS_DATAFIELD_MAPPING[field]] = b.pop(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167589"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset['match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somehow the data must be split up: one part has to be use to call this gazetteer.sample function,\n",
    "# the other part is used as the \"real\" trainings data:\n",
    "\n",
    "NUM_FOR_SAMPLING = 1000\n",
    "NUM_FOR_TRAINING = 5000\n",
    "\n",
    "messy = {('d'+str(k)):v[0] for k,v in zip(range(NUM_FOR_SAMPLING), trainset['distinct'])}\n",
    "canonical = {('m'+str(k)):v[0] for k,v in zip(range(NUM_FOR_SAMPLING), trainset['match'])}\n",
    "\n",
    "new_trainset = {'match':trainset['match'][NUM_FOR_SAMPLING+1:NUM_FOR_SAMPLING + NUM_FOR_TRAINING], \n",
    "                'distinct':trainset['distinct'][NUM_FOR_SAMPLING+1:NUM_FOR_SAMPLING + NUM_FOR_TRAINING]}\n",
    "new_trainset_filepath = str(Path(os.getenv('DATA_PATH')) / 'raw' / 'tmp_trainset.json')\n",
    "with open(new_trainset_filepath, 'w') as fp:\n",
    "    json.dump(new_trainset, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:reading training from file\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, name), SimplePredicate: (sameSevenCharStartPredicate, address.city))\n",
      "INFO:dedupe.training:(SimplePredicate: (doubleMetaphone, name), SimplePredicate: (sameSevenCharStartPredicate, address.city))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonTwoTokens, address.street), SimplePredicate: (sameSevenCharStartPredicate, address.city))\n",
      "INFO:dedupe.training:(SimplePredicate: (sameSevenCharStartPredicate, address.city), SimplePredicate: (sortedAcronym, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (sameFiveCharStartPredicate, address.postal_code), SimplePredicate: (suffixArray, name))\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.001000, score 0.9934036894499885\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (doubleMetaphone, name), SimplePredicate: (sameSevenCharStartPredicate, address.city))\n",
      "INFO:dedupe.training:(SimplePredicate: (oneGramFingerprint, name), SimplePredicate: (sameSevenCharStartPredicate, address.city))\n"
     ]
    }
   ],
   "source": [
    "gazetteer = dedupe.Gazetteer(fields)\n",
    "gazetteer.sample(messy, canonical)\n",
    "\n",
    "with open(new_trainset_filepath) as tf:\n",
    "    gazetteer.readTraining(tf)\n",
    "    \n",
    "gazetteer.train()\n",
    "\n",
    "model_output_filepath = str(Path(os.getenv('MODELS_PATH')) / f'201904120940_big_kantwert_S{NUM_FOR_SAMPLING}_T{NUM_FOR_TRAINING}.model')\n",
    "with open(model_output_filepath, 'wb') as sf:\n",
    "    gazetteer.writeSettings(sf, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
